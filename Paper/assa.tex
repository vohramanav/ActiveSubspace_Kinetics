\section{Active subspaces and sensitivity analysis}
\label{sec:assa}
%In this section, we provide key elements of the active subspace methodology and
%provide theoretical connections between the global sensitivity measures
%(DGSMs), activity scores, and the total Sobol' indices.

\subsection{Active subspaces}
\label{sub:ac}
As mentioned in the introduction, the active subspace is low-dimensional subspace
that constitutes important directions in a model's input
space~\cite{Constantine:2015}. The effective variability in the model output, $f$
due to uncertain inputs, $\bm{\theta}$ in the physical space, is predominantly captured
along these directions. Each component of $\bm{\theta}$
is mapped to a corresponding value in $\Omega\in\mathbb{R}^{N_p}$
($N_p$: number of uncertain inputs)
using a linear transformation.  The set of canonical random variables, corresponding
to $\theta_i$'s, in $\Omega$ is referred to as $\bm{\xi}$.
Hence, from this point onwards, we denote $f$ as $f(\bm{\xi})$.
Mathematically, for a given $f$, and a set of inputs, $\bm{\xi}$,
these directions are the eigenvectors of a positive
semi-definite matrix, $\mat{C}$, defined as follows: 
%
\be
\mat{C} = \int_\Omega (\nabla_{\bm{\xi}}f)(\nabla_{\bm{\xi}}f)^\top dP_{\bm{\xi}}
\label{eq:C}
\ee
%
where 
$dP_{\bm{\xi}}$ = $p(\bm{\xi})d\bm{\xi}$, where $p(\bm{\xi})$ is the joint probability
distribution of $\bm{\xi}$. Note that the function, $f$ is considered to be continuous 
and differentiable in the physical space, $\bm{\theta}$. 
%Hence, it is possible that a given $f(\bm{\xi})$ might not admit an active
%subspace. However, it is of remarkable interest to investigate if one exists
%since the subspace could be exploited to reduce the dimensionality of the
%problem and hence the associated computational effort.
Here 
$\nabla_{\bm{\xi}}f$ denotes the gradient vector. 
%with individual components
%being partial derivatives of $f$ with respect to the $i^\text{th}$ input, $\xi_i$. 
Since $\mat{C}$ is symmetric and
positive semi-definite, it admits a spectral decomposition:
%
\be
\mat{C} = \mat{W}\mat{\Lambda}\mat{W}^\top
\ee
%
where $\mat{\Lambda}$ = diag($\lambda_1,\ldots,\lambda_{N_p}$) with
$\lambda_i$'s sorted in descending order
\[
     \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_\Np \geq 0.
\] 
The eigenpairs are partitioned about the $i^{\text{th}}$ eigenvalue such that
the ratio, $\left(\frac{\lambda_i}{\lambda_{i+1}}\right)$ $\gg$ 1 as follows:
\be
 \mat{W} = [\mat{W}_1~\mat{W}_2],~~\mat{\Lambda} = \begin{bmatrix}\mat{\Lambda}_1 & \\  &
  \mat{\Lambda}_2 \\
\end{bmatrix}
\ee
 %
 where $\mat{W}_1$ comprises the dominant eigenspace of $\mat{C}$ regarded as
the active subspace, and $\mat{\Lambda}_1$ is the corresponding set of
eigenvalues. Once the active subspace is computed, dimension reduction is
accomplished by transforming the set of canonical random variables, $\bm{\xi}$
into a reduced set, $\mat{W}_1^\top \bm{\xi}$, referred to as the set of active
variables. The number of active variables is thus equal to the number
of dominant eigenvectors. The model output in the original space, $f(\bm{\xi})$,
is expressed in terms of the active variables as $G(\mat{W}_1^\top \bm{\xi})$.
Hence, $f$ is essentially evaluated at the projection of $\bm{\xi}$ on the
column space of $\mat{W}_1$. We could thus confine UQ to the inputs in the
active subspace whose dimension is typically much smaller (in applications that
admit such a subspace) than the dimension of the original input space. To 
expedite UQ, one could easily fit a regression-surface to $G$ using the 
following sequence of steps outlined in~\cite{Constantine:2015} (see chapter 4).
\begin{enumerate}
\item Consider a set of input-output pairs, $\{\bm{\xi}_i, f(\bm{\xi}_i)\}$.
\item For each $\bm{\xi}_i$, compute $\mat{W}_1^\top\bm{\xi}_i$. Note that
 $G(\mat{W}_1^\top \bm{\xi})$ $\approx$ $f(\bm{\xi}_i)$.
\item Fit a regression surface, $\tilde{G}(\mat{W}_1^\top\bm{\xi}_i)$ $\approx$ 
$G(\mat{W}_1^\top \bm{\xi})$.
\item Overall approximation, $f(\bm{\xi})$ $\approx$ $\tilde{G}(\mat{W}_1^\top\bm{\xi})$.
\end{enumerate}

In practice, the matrix, $\mat{C}$ is typically
approximated using pseudo-random sampling techniques such as Monte Carlo and
Latin hypercube sampling. The integral in~\eqref{eq:C} is replaced with a
summation as follows:
 %
 \be
 \mat{C}\approx \hat{\mat{C}} = \frac{1}{N}\sum\limits_{i=1}^{N} 
 (\nabla_{\bm{\xi}}f(\bm{\xi}_i))(\nabla_{\bm{\xi}}f(\bm{\xi}_i))^\top
 = \hat{\mat{W}}\hat{\mat{\Lambda}}\hat{\mat{W}}^\top
\label{eq:chat}
 \ee
 %
Clearly, the amount of computational effort associated with constructing the matrix,
$\hat{\mat{C}}$ scales with the number of samples, $N$. Hence, an iterative
computational approach is adopted in this work to ensure that $N$ is sufficiently
large so that the dominant eigenvectors in $\hat{\mat{W}}_1$ are approximated
with reasonable accuracy as discussed later in Section~\ref{sec:method}. 

%%It has been shown that the accuracy of approximated dominant eigenspace,
%%$\hat{\bm{w}}_1$ is inversely proportional to the difference between the
%%smallest eigenvalue in $\hat{\bm{\Lambda}}_1$ and the largest eigenvalue in
%%$\hat{\bm{\Lambda}}_2$~\cite{Constantine:2014}. 
%%Components of the eigenvectors in the active subspace could be used for
%%estimating the so-called activity scores as a measure for global sensitivity
%%and also be used for approximating the DGSMs as discussed in the following
%%section.
  

\subsection{Global sensitivity measures}

\label{sub:gsa}
Consider a function $f(\xi_1, \xi_2, \ldots, \xi_\Np)$. 
Whereas the active subspace framework described above not make any assumptions
about statistical independence of the input parameters $\xi_i$, $i = 1, \ldots, \Np$, 
the classical 
framework of variance based sensitivity analysis~\cite{Sobol:2001, Saltelli:2010} assumes the input
parameters are statistically independent. While extensions to the cases 
of correlated inputs exist, we limit the discussion in this section to the
case of random inputs that are statistically independent and are 
either uniformly distributed or
distributed according to a Boltzmann probability distribution.
Note that a measure $\mu$
on $\R$ is referred to as a Boltzmann measure if it is 
absolutely continuous with respect to the Lebesgue measure  
and admits a density  of the form $\pi(x) = C \exp\{-V(x)\}$,
where $V$ is a continuous function and $C$ a normalization 
constant~\cite{Lamboni:2013}.


The total Sobol' index ($T_i(f)$) of a model output, $f(\vec\theta)$ quantifies
the total contribution of the input, $\theta_i$ to the variance of the
output~\cite{Sobol:2001}. Mathematically, this can be expressed as follows:
%
\be
T_i(f) = 1 - 
\frac{\V[\mathbb{E}(f|\bm{\theta}_{\sim i})]}{\V(f)},
\label{eq:total}
\ee
%
where $\bm{\theta}_{\sim i}$ is a vector of all but $i^\text{th}$ uncertain
input, and $\V$ denotes the variance. The total Sobol' index thus accounts
for the contribution of a given input to the variability in the output by itself
as well as due to its interaction or coupling with other inputs. 
Determining accurate estimates of $T_i(f)$ typically involves tens of
thousands of model runs and is therefore prohibitive in the case of
compute-intensive applications. The DGSMs~\cite{Sobol:2009} provide a means for
approximating the upper bound on $T_i(f)$ with fewer computations; see 
also~\cite{Vohra:2018}. 

For $f: \Omega \to \R$, we consider the DGSMs,
\[
    \nu_i(f) := \E{\left(\frac{\partial f}{\partial\xi_i}\right)^2} =
                  \int_\Omega 
                  \left(\frac{\partial f}{\partial\xi_i}\right)^2
                  \pi(\vec{\xi})d\vec{\xi}, \quad i = 1, \ldots, \Np.   
\]
Here $\pi$ is the joint PDF of $\xi$. 
Let $\mat{C} = \E{\nabla f \nabla f^\top}$ and note that
\[
   \nu_i(f) = \vec{e}_i^\top \mat{C} \vec{e}_i, 
\]
where $\vec{e}_i$ is the $i$th coordinate vector in $\R^\Np$.
This also shows that $\sum_{j=1}^\Np \nu_i(f) = \trace(\mat{C})$.
We consider the spectral decomposition of $\mat{C}$, given by  
$\mat{C} = \sum_{k=1}^\Np \lambda_k \vec{u}_k \vec{u}_k^\top$, where 
$\lambda_i$ are the (non-negative) eigenvalues of $\mat{C}$, in descending
order, and
$\vec{u}_k$ are the corresponding (orthonormal) eigenvectors.
We note that,
\begin{equation}\label{equ:spectral_DGSM}
\nu_i(f) = \vec{e}_i^T \Big(\sum_{k=1}^\Np \lambda_k \vec{u}_k \vec{u}_k^\top\Big) \vec{e}_i
 = \sum_{k=1}^\Np \lambda_k \ip{\vec{e}_i}{\vec{u}_k}^2, 
\end{equation}
which gives a spectral representation for the DGSMs. 
In the case the eigenvalues decay rapidly to zero, we can obtain
accurate approximations of $\nu_i(f)$ by Truncating the summation: 
\newcommand{\act}[3]{\nu_{{#2},{#3}}({#1})}
\newcommand{\actt}[3]{\tilde{\nu}_{{#2},{#3}}({#1})}
\[
   \act{f}{i}{r} =  \sum_{k=1}^r \lambda_k \ip{\vec{e}_i}{\vec{u}_k}^2,
   \quad j = 1, \ldots, \Np, \quad r \leq \Np.
\]
The quantities $\act{f}{i}{r}$ are called activity scores
in~\cite{Diaz:2016,Constantine:2017}.
%The activity scores connect ideas from active subspaces and global sensitivity
%analysis, and can be used to approximate DGSMs.  
The following result, which
can also be found in~\cite{Diaz:2016,Constantine:2017}, quantifies the error in this
approximation. We provide a short proof for completeness. 
\begin{proposition}\label{prp:dgsm_bound} 
For $1 \leq r \leq \Np$,
\[
0 \leq \nu_i(f) - \act{f}{i}{r} \leq \lambda_{r+1}, \quad i = 1, \ldots, \Np.
\] 
\end{proposition}
\begin{proof} 

%Using the spectral representation of the DGSMs and the definition of activity
%scores we clearly see:
%\[
%\alpha_i(f;r) = \sum_{k=1}^{r}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 \leq \sum_{k=1}^{N_p}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 = \nu_i(f), \quad \quad i = 1,\ldots,N_p, \quad r \leq N_p
%\]
%In other words
%\[
%0 \leq  \sum_{k=1}^{N_p}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 - \sum_{k=1}^{r}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2,  \quad \quad i = 1,\ldots,N_p
%\]
%with equality if $N_p=r$.
%\newline
%We can write:
%\[
%\begin{aligned}
%\nu_i(f) = \sum_{k=1}^{N_p}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 = \sum_{k=1}^{r}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 + \sum_{k=r+1}^{N_p}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2  \\
%= \alpha_i(f;r) + \sum_{k=r+1}^{N_p}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 \leq \alpha_i(f;r) + \lambda_{r+1} \sum_{k=r+1}^{N_p} \langle \vec{e}_i, \vec{u}_k \rangle^2,  \quad \quad i = 1,\ldots,N_p
%\end{aligned}
%\]
%The eigenvectors $\vec{u}_k$ are orthonormal so they all have length 1. Also note that for every $x \in \R^n$ we have \[ \norm{x}^2 = \sum_{k=1}^{n} \langle \vec{x}, \vec{u}_k \rangle^2\] This is known as Parseval's identity.
%In particular in this case $\vec{x} = \vec{e}_i \in \R^{N_p}$ so
%\[1 = \norm{\vec{e}_i}^2 = \sum_{k=r+1}^{N_p} \langle \vec{e}_i, \vec{u}_k \rangle^2\]
%Finally we write:
%\[
%\nu_i(f) \leq \alpha_i(f;r) + \lambda_{r+1}, \quad \quad i = 1,\ldots,N_p
%\]

Note that, $\nu_i(f) - \act{f}{i}{r}= \sum_{k=r+1}^\Np \lambda_k \ip{\vec{e}_i}{\vec{u}_k}^2 \geq 0$,
which gives the first inequality. To see the upper bound, we note,
\[
   \sum_{k=r+1}^\Np \lambda_k \ip{\vec{e}_i}{\vec{u}_k}^2 \leq \lambda_{r+1} \sum_{k=r+1}^\Np \ip{\vec{e}_i}{\vec{u}_k}^2
   \leq \lambda_{r+1}. 
\]
The last inequality holds because 
$1 = \|\vec{e}_i\|_2^2 = 
\sum_{k = 1}^\Np \ip{\vec{e}_i}{\vec{u}_k}^2 
\geq \sum_{k=r+1}^\Np \ip{\vec{e}_i}{\vec{u}_k}^2$.
\end{proof} 
The utility of this result is realized in problems with 
high-dimensional parameter spaces in which 
the eigenvalues $\lambda_i$ decay rapidly to zero; in 
such cases, this result implies that  $\nu_i(f) \approx \act{f}{i}{r}$,
where $r$ is the \emph{numerical rank} of $\mat{C}$.  This will be especially
effective if there is a large gap in the eigenvalues.  


It was shown in~\cite{Lamboni:2013} that the total Sobol' 
index $T_i(f)$ can be bounded in terms of $\nu_i(f)$:
\begin{equation}\label{equ:sobol_bound}
T_i(f) \leq \frac{C_i}{\V(f)}\nu_i(f), \quad i = 1, \ldots, \Np,
\end{equation}
where for each $i$, $C_i$ is an appropriate \emph{Poincar\'{e}} constant
that depends on the distribution of $\xi_i$.
This provides a strong theoretical basis for using DGSMs to identify 
unimportant inputs. Moreover, 
in~\cite{Vohra:2018} we introduced the normalized screening metric
\[
   \tilde{\nu}_i(f) = \frac{C_i \nu_i(f)}{\sum_{i=1}^\Np C_i \nu_i(f)},
\]
for detecting unimportant input parameters. 
Note that if $\xi_i$ are iid, the $C_i$'s will cancel and the normalized screening
metric reduces to 
\[
    \tilde{\nu}_i(f) = \frac{\nu_i(f)}{\sum_{i=1}^\Np \nu_i(f)} 
      = \frac{\vec{e}_i^T \mat{C} \vec{e}_i}{\trace(\mat{C})} 
      = \frac{\sum_{k=1}^\Np \lambda_k \ip{\vec{e}_i}{\vec{u}_k}^2}{\sum_{k = 1}^\Np \lambda_k}.
\]

We can also bound the normalized DGSMs using activity scores; it is easy to see  
\[
\tilde{\nu}_i(f) \leq 
\frac{ C_i \big(\act{f}{i}{r} + \lambda_{r+1}\big)}{\sum_{i=1}^\Np C_i \act{f}{i}{r}}
=\frac{C_i \act{f}{i}{r}}{\sum_{i=1}^\Np C_i \act{f}{i}{r}} + \kappa_i \lambda_{r+1}, 
\]
with $\kappa_i = C_i / (\sum_i C_i \act{f}{i}{r})$. 
In the case where where $\lambda_{r+1} \approx 0$, 
this motivates definition of
normalized activity scores
\[
   \actt{f}{i}{r} =  \frac{C_i \act{f}{i}{r}}{\sum_{i=1}^\Np C_i \act{f}{i}{r}}.
\] 

Combining Proposition~\ref{prp:dgsm_bound} and~\eqref{equ:sobol_bound}, shows
an interesting link between the activity scores and total Sobol' indices.
Specifically, by computing the activity scores, we can identify unimportant
parameters.  
One can attempt to reduce parameter dimension by fixing
unimportant parameters at nominal values. Suppose $\xi_i$ is
deemed unimportant as a result of a global sensitivity analysis
based on computing the activity scores. We want to understand
what is the approximation error that occurs once we fix $\xi_i$ 
at a nominal value.
%Let $\I \subset \{1, \ldots,
%Np\}$ be an index set that indexes the unimportant parameters in the parameter
%vector $\vec{\xi}$.  Letting $|\I|$ denote the number of elements of $\I$, 
%we denote by $\vec{z} \in \R^{|\I|}$ a generic vector of nominal values for
%the unimportant parameters $\vec\xi_\I$, and let $\vec{y}$ be a generic vector of
%important variables, $\vec\xi_{\I^c}$,
%where $\I^c$ denotes the complement of $\I$ in $\{1, \ldots, \Np\}$. 
%For a vector $\xi$, we let $\xin$ denote the vector obtained by removing
%$\xi_i$. 

Let $\vec\xi$ be given and let $z$ be a nominal value for $\xi_i$;  we define 
$\vec{y}^z(\vec\xi)$ as the vector with entries $y^z_j = \xi_j$ for $j \neq i$
and $y^z_i = z_i$.
We consider the \emph{reduced} model, 
obtained by fixing $\xi_i$ at a nominal value $z \in \R$: 
\[
f^{z}(\vec\xi) = f(\vec{y}^z(\vec\xi))
\] 
We consider the following error indicator.
\[
\delta(z) =
\frac{ \int_\Omega \big( f(\vec\xi) - f^{z}(\vec\xi)\big)^2 \, \mu(d\vec\xi) }
          {\int_\Omega f(\vec\xi)^2 \, \mu(d\vec\xi)}.
\] 

\begin{theorem}
We have $\Ez{ \delta(z)} \leq 2C_i\big(\act{f}{i}{r} + \lambda_{r+1}\big)/{\V(f)} $.
\end{theorem}
%
%\[
%       T_i(f) \leq \frac{C_i}{\V(f)}(\act{f}{i}{r} + \lambda_{r+1}).
%\]
%we obtain the following useful inequality, which
%links activity scores and Sobol' indices:
%for $1 \leq r \leq \Np$, and $i = 1, \ldots, \Np$, we have
%\[
%       T_i(f) \leq \frac{C_i}{\V(f)}(\act{f}{i}{r} + \lambda_{r+1}).
%\]
%This was noted in~\cite{Diaz:2016} for the specific case of all $\xi_i$'s being
%iid $U(0, 1)$, in which case $C_i = 1/\pi^2$, $i = 1, \ldots, \Np$.
\begin{proof} \hayleynote{(NOT FINISHED YET, NEED TO SORT OUT INDEXING TO BE IN LINE WITH THE PAPER)}
First we show the numerator 
\[\int_\Omega \big( f(\vec\xi) - f^{z}(\vec\xi)\big)^2 \, \mu(d\vec\xi) = 2\V_{U^c}(f) \]
Using the ANOVA decomposition we write:
\[ f(\vec\xi) = f_0 + f_1(\vec\xi_{U}) + f_2(\vec\xi_{U^C}) + f_{12}(\vec\xi_U,\vec\xi_{U^C})\]
Then we can simplify the numerator as follows:
\[ \int_{\Omega} \big( f_2(\vec\xi_{U^C}) + f_{12}(\vec\xi_{U},\vec\xi_{U^C}) - f_2(z) -f_{12}(\vec\xi_{U},z)\big)^2\mu(d\vec\xi) \]
Using the properties of ANOVA:
\[
\int_{\Omega_{U^c}} f_2(\vec\xi_{{U^C}}) \,\mu(d\vec\xi_{U^C}) 
= \int_{\Omega_{U^C}} f_{12}(\vec\xi_U, \vec\xi_{U^C})\,\mu(d\vec\xi_{U^C}) 
= \int_{\Omega_U} f_{12}(\vec\xi_U, \vec\xi_{U^C})\,\mu(d\vec\xi_U) = 0,
\]
For a fixed $z \in \Omega_{U^C}$ we can simplify the numerator further:
\[ \int_{\Omega}  f_2^2(\vec\xi_{U^C}) + f_{12}^2(\vec\xi_{U},\vec\xi_{U^C}) - f_2^2(z) -f_{12}^2(\vec\xi_{U},z) \mu(d\vec\xi) = \]
\[ \V_{U^C}(f) + \V_{U^C,U}(f) + f_2^2(z) + \int_{\Omega_U} f_{12}^2(\vec\xi_U, z) \mu(d\vec\xi_U),\]
Taking the expectation of the above expression over $\Omega_{U^c}$ gives 
desired result:
\[\V_{U^C}(f) + \V_{U^C,U}(f) + \int_{\Omega_{U^C}} f_2^2(z) \, \mu(dz)
+ \int_{\Omega_{U^C}}\int_{\Omega_U} f_{12}^2(\vec\xi_U,z)\,\mu (d\vec\xi_U)
\mu (dz) \]
\[= \V_{U^C}(f) + \V_{U,U^C}(f) + \V_{U^C}(f) + \V_{U,U^c}(f) 
= 2 \V_{U^c}^\text{tot}(f) \]
Using the definition of variance we rewrite the the denominator as:
\[
 \int_\Omega f(\vec\xi)^2\, \mu(d\vec\xi) 
=  \V(f) + 
\Big(\int_\Omega f(\vec\xi)\mu(d\vec\xi)\Big)^2 
\geq   \V(f) 
\]	
Combining by parts we have:
\[ \Ez{ \delta(z)} = 2T_{U^C}(f) \]
Using (10) above yields:
\[2T_{U^C}(f) \leq \frac{C_{U^C}}{\V(f)}\nu_{U^C}(f) \]
Finally using Proposition 2.1 gives us the result:
\[2T_{U^C}(f) \leq 2\frac{C_{U^C}}{\V(f)}\nu_{U^C}(f) \leq 2\frac{C_{U^C}}{\V(f)}(\nu_{{U^C},r}(f) + \lambda_{r+1} ) \]
\end{proof}


