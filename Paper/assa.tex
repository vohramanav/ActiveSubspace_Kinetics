\section{Active subspaces and sensitivity analysis}
\label{sec:assa}
%In this section, we provide key elements of the active subspace methodology and
%provide theoretical connections between the global sensitivity measures
%(DGSMs), activity scores, and the total Sobol' indices.

\subsection{Active subspaces}
\label{sub:ac}
As mentioned in the introduction, the active subspace is low-dimensional subspace
that constitutes important directions in a model's input
space~\cite{Constantine:2015}. The effective variability in the model output
due to uncertain inputs is predominantly captured along these directions.
Mathematically, for a given model output, $f(\bm{\xi})$, and a set of inputs, $\bm{\xi}$,
these directions are the eigenvectors of a positive
semi-definite matrix, $\mat{C}$, defined as follows: 
%
\be
\mat{C} = \int_\Omega (\nabla_{\bm{\xi}}f)(\nabla_{\bm{\xi}}f)^\top dP_{\bm{\xi}}
\label{eq:C}
\ee
%
where $\Omega\in\mathbb{R}^{N_p}$ is the domain of $\bm{\xi}$, and $N_p$ is the
number of uncertain inputs. 
$dP_{\bm{\xi}}$ = $p(\bm{\xi})d\bm{\xi}$, where $p(\bm{\xi})$ is the joint probability
distribution of $\bm{\xi}$. Note that the function, $f$ is considered to be continuous 
and differentiable in the considered parameter domain. 
%Hence, it is possible that a given $f(\bm{\xi})$ might not admit an active
%subspace. However, it is of remarkable interest to investigate if one exists
%since the subspace could be exploited to reduce the dimensionality of the
%problem and hence the associated computational effort.
Here 
$\nabla_{\bm{\xi}}f$ denotes the gradient vector. 
%with individual components
%being partial derivatives of $f$ with respect to the $i^\text{th}$ input, $\xi_i$. 
Since $\mat{C}$ is symmetric and
positive semi-definite, it admits a spectral decomposition:
%
\be
\mat{C} = \mat{W}\mat{\Lambda}\mat{W}^\top
\ee
%
where $\mat{\Lambda}$ = diag($\lambda_1,\ldots,\lambda_{N_p}$) with
$\lambda_i$'s sorted in descending order
\[
     \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_\Np \geq 0.
\] 
The eigenpairs are partitioned about the $i^{\text{th}}$ eigenvalue such that
the ratio, $\left(\frac{\lambda_i}{\lambda_{i+1}}\right)$ $\gg$ 1 as follows:
\be
 \mat{W} = [\mat{W}_1~\mat{W}_2],~~\mat{\Lambda} = \begin{bmatrix}\mat{\Lambda}_1 & \\  &
  \mat{\Lambda}_2 \\
\end{bmatrix}
\ee
 %
 where $\mat{W}_1$ comprises the dominant eigenspace of $\mat{C}$ regarded as
the active subspace, and $\mat{\Lambda}_1$ is the corresponding set of
eigenvalues. 

\alennote{The rest of this paragraph needs to be rewritten, as it is hard to 
see what is going on; what is $\theta$? I suggest, mentioning $\vec\theta$ 
and $\vec\xi$ earlier, and from then on make everything function of $\vec\xi$.
Explain the remaining ideas more clearly.}
The active subspace is used to construct the so-called active
variables, $\mat{W}_1^\top \bm{\xi}$ and $f(\bm{\xi}(\theta))$ is transformed as
$G(\mat{W}_1^\top \bm{\xi})$. In practice, the set of samples, $\bm{\xi}_i$'s
are projected to the physical space as $\bm{\theta}_i$'s at which the model
output, $f(\bm{\xi}(\theta))$ is computed.  The matrix, $\mat{C}$ is typically
approximated using pseudo-random sampling techniques such as Monte Carlo and
Latin hypercube sampling. The integral in~\eqref{eq:C} is replaced with a
summation as follows:
 %
 \be
 \mat{C}\approx \hat{\mat{C}} = \frac{1}{N}\sum\limits_{i=1}^{N} 
 (\nabla_{\bm{\xi}}f(\bm{\xi}_i))(\nabla_{\bm{\xi}}f(\bm{\xi}_i))^\top
 = \hat{\mat{W}}\hat{\mat{\Lambda}}\hat{\mat{W}}^\top
\label{eq:chat}
 \ee
 %
%%It has been shown that the accuracy of approximated dominant eigenspace,
%%$\hat{\bm{w}}_1$ is inversely proportional to the difference between the
%%smallest eigenvalue in $\hat{\bm{\Lambda}}_1$ and the largest eigenvalue in
%%$\hat{\bm{\Lambda}}_2$~\cite{Constantine:2014}. 
%%Components of the eigenvectors in the active subspace could be used for
%%estimating the so-called activity scores as a measure for global sensitivity
%%and also be used for approximating the DGSMs as discussed in the following
%%section.
  

\subsection{Global sensitivity measures}

\label{sub:gsa}
Consider a function $f(\xi_1, \xi_2, \ldots, \xi_\Np)$. 
Whereas the active subspace framework described above not make any assumptions
about statistical independence of the input parameters $\xi_i$, $i = 1, \ldots, \Np$, 
the classical 
framework of variance based sensitivity analysis~\cite{MMM} assumes the input
parameters are statistically independent. While extensions to the cases 
of correlated inputs exist, we limit the discussion in this section to the
case of random inputs that are statistically independent and are 
either uniformly distributed or
distributed according to a Boltzmann probability distribution~\alennote{Explain what this
is, and give a reference}.  

The total Sobol' index ($T_i(f)$) of a model output, $f(\vec\theta)$ quantifies
the total contribution of the input, $\theta_i$ to the variance of the
output.\alennote{references} Mathematically, this can be expressed as follows:
%
\be
T_i(f) = 1 - 
\frac{\V[\mathbb{E}(f|\bm{\theta}_{\sim i})]}{\V(f)},
\label{eq:total}
\ee
%
where $\bm{\theta}_{\sim i}$ is a vector of all but $i^\text{th}$ uncertain
input, and $\V$ denotes the variance (~\alennote{Explain this more clearly.}).
Determining accurate estimates of $T_i(f)$ typically involves tens of
thousands of model runs and is therefore prohibitive in the case of
compute-intensive applications. The DGSMs~\cite{Sobol:2009} provide a means for
approximating the upper bound on $T_i(f)$ with fewer computations; see 
also~\cite{Vohra:2018}. 

For $f: \Omega \to \R$, we consider the DGSMs,
\[
    \nu_i(f) := \E{\left(\frac{\partial f}{\partial\xi_i}\right)^2} =
                  \int_\Omega 
                  \left(\frac{\partial f}{\partial\xi_i}\right)^2
                  \pi(\vec{\xi})d\vec{\xi}, \quad i = 1, \ldots, \Np.   
\]
Here $\pi$ is the joint PDF of $\xi$. 
Let $\mat{C} = \E{\nabla f \nabla f^\top}$ and note that
\[
   \nu_i(f) = \vec{e}_i^\top \mat{C} \vec{e}_i, 
\]
where $\vec{e}_i$ is the $i$th coordinate vector in $\R^\Np$.
This also shows that $\sum_{j=1}^\Np \nu_i(f) = \trace(\mat{C})$.
We consider the spectral decomposition of $\mat{C}$, given by  
$\mat{C} = \sum_{k=1}^\Np \lambda_k \vec{u}_k \vec{u}_k^\top$, where 
$\lambda_i$ are the (non-negative) eigenvalues of $\mat{C}$, in descending
order, and
$\vec{u}_k$ are the corresponding (orthonormal) eigenvectors.
We note that,
\begin{equation}\label{equ:spectral_DGSM}
\nu_i(f) = \vec{e}_i^T \Big(\sum_{k=1}^\Np \lambda_k \vec{u}_k \vec{u}_k^\top\Big) \vec{e}_i
 = \sum_{k=1}^\Np \lambda_k \ip{\vec{e}_i}{\vec{u}_k}^2, 
\end{equation}
which gives a spectral representation for the DGSMs. 
In the case the eigenvalues decay rapidly to zero, we can obtain
accurate approximations of $\nu_i(f)$ by Truncating the summation: 
\newcommand{\act}[3]{\nu_{{#2},{#3}}({#1})}
\newcommand{\actt}[3]{\tilde{\nu}_{{#2},{#3}}({#1})}
\[
   \act{f}{i}{r} =  \sum_{k=1}^r \lambda_k \ip{\vec{e}_i}{\vec{u}_k}^2,
   \quad j = 1, \ldots, \Np, \quad r \leq \Np.
\]
The quantities $\act{f}{i}{r}$ are called activity scores
in~\cite{Diaz:2016,Constantine:2017}.
%The activity scores connect ideas from active subspaces and global sensitivity
%analysis, and can be used to approximate DGSMs.  
The following result, which
can also be found in~\cite{Diaz:2016,Constantine:2017}, quantifies the error in this
approximation. We provide a short proof for completeness. 
\begin{proposition}\label{prp:dgsm_bound} 
For $1 \leq r \leq \Np$,
\[
0 \leq \nu_i(f) - \act{f}{i}{r} \leq \lambda_{r+1}, \quad i = 1, \ldots, \Np.
\] 
\end{proposition}
\begin{proof} 

%Using the spectral representation of the DGSMs and the definition of activity
%scores we clearly see:
%\[
%\alpha_i(f;r) = \sum_{k=1}^{r}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 \leq \sum_{k=1}^{N_p}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 = \nu_i(f), \quad \quad i = 1,\ldots,N_p, \quad r \leq N_p
%\]
%In other words
%\[
%0 \leq  \sum_{k=1}^{N_p}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 - \sum_{k=1}^{r}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2,  \quad \quad i = 1,\ldots,N_p
%\]
%with equality if $N_p=r$.
%\newline
%We can write:
%\[
%\begin{aligned}
%\nu_i(f) = \sum_{k=1}^{N_p}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 = \sum_{k=1}^{r}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 + \sum_{k=r+1}^{N_p}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2  \\
%= \alpha_i(f;r) + \sum_{k=r+1}^{N_p}\lambda_k \langle \vec{e}_i, \vec{u}_k \rangle^2 \leq \alpha_i(f;r) + \lambda_{r+1} \sum_{k=r+1}^{N_p} \langle \vec{e}_i, \vec{u}_k \rangle^2,  \quad \quad i = 1,\ldots,N_p
%\end{aligned}
%\]
%The eigenvectors $\vec{u}_k$ are orthonormal so they all have length 1. Also note that for every $x \in \R^n$ we have \[ \norm{x}^2 = \sum_{k=1}^{n} \langle \vec{x}, \vec{u}_k \rangle^2\] This is known as Parseval's identity.
%In particular in this case $\vec{x} = \vec{e}_i \in \R^{N_p}$ so
%\[1 = \norm{\vec{e}_i}^2 = \sum_{k=r+1}^{N_p} \langle \vec{e}_i, \vec{u}_k \rangle^2\]
%Finally we write:
%\[
%\nu_i(f) \leq \alpha_i(f;r) + \lambda_{r+1}, \quad \quad i = 1,\ldots,N_p
%\]

Note that, $\nu_i(f) - \act{f}{i}{r}= \sum_{k=r+1}^\Np \lambda_k \ip{\vec{e}_i}{\vec{u}_k}^2 \geq 0$,
which gives the first inequality. To see the upper bound, we note,
\[
   \sum_{k=r+1}^\Np \lambda_k \ip{\vec{e}_i}{\vec{u}_k}^2 \leq \lambda_{r+1} \sum_{k=r+1}^\Np \ip{\vec{e}_i}{\vec{u}_k}^2
   \leq \lambda_{r+1}. 
\]
The last inequality holds because 
$1 = \|\vec{e}_i\|_2^2 = 
\sum_{k = 1}^\Np \ip{\vec{e}_i}{\vec{u}_k}^2 
\geq \sum_{k=r+1}^\Np \ip{\vec{e}_i}{\vec{u}_k}^2$.
\end{proof} 
The utility of this result is realized in problems with 
high-dimensional parameter spaces in which 
the eigenvalues $\lambda_i$ decay rapidly to zero; in 
such cases, this result implies that  $\nu_i(f) \approx \act{f}{i}{r}$,
where $r$ is the \emph{numerical rank} of $\mat{C}$.  This will be especially
effective if there is a large gap in the eigenvalues.  


It was shown in~\cite{Lamboni:2013} that the total Sobol' 
index $T_i(f)$ can be bounded in terms of $\nu_i(f)$:
\begin{equation}\label{equ:sobol_bound}
T_i(f) \leq \frac{C_i}{\V(f)}\nu_i(f), \quad i = 1, \ldots, \Np,
\end{equation}
where for each $i$, $C_i$ is an appropriate \emph{Poincar\'{e}} constant.
This provides a strong theoretical basis for using DGSMs to identify 
unimportant inputs. Moreover, 
in~\cite{Vohra:2018} we introduced the normalized screening metric
\[
   \tilde{\nu}_i(f) = \frac{C_i \nu_i(f)}{\sum_{i=1}^\Np C_i \nu_i(f)},
\]
for detecting unimportant input parameters. 
Note that if $\xi_i$ are iid, the $C_i$'s will cancel and the normalized screening
metric reduces to 
\[
    \tilde{\nu}_i(f) = \frac{\nu_i(f)}{\sum_{i=1}^\Np \nu_i(f)} 
      = \frac{\vec{e}_i^T \mat{C} \vec{e}_i}{\trace(\mat{C})} 
      = \frac{\sum_{k=1}^\Np \lambda_k \ip{\vec{e}_i}{\vec{u}_k}^2}{\sum_{k = 1}^\Np \lambda_k}.
\]

We can also bound the normalized DGSMs using activity scores; it is easy to see  
\[
\tilde{\nu}_i(f) \leq 
\frac{ C_i \big(\act{f}{i}{r} + \lambda_{r+1}\big)}{\sum_{i=1}^\Np C_i \act{f}{i}{r}}
=\frac{C_i \act{f}{i}{r}}{\sum_{i=1}^\Np C_i \act{f}{i}{r}} + \kappa_i \lambda_{r+1}, 
\]
with $\kappa_i = C_i / (\sum_i C_i \act{f}{i}{r})$. 
In the case where where $\lambda_{r+1} \approx 0$, 
this motivates definition of
normalized activity scores
\[
   \actt{f}{i}{r} =  \frac{C_i \act{f}{i}{r}}{\sum_{i=1}^\Np C_i \act{f}{i}{r}}.
\] 

Combining Proposition~\ref{prp:dgsm_bound} and~\eqref{equ:sobol_bound}, shows
an interesting link between the activity scores and total Sobol' indices.
Specifically, by computing the activity scores, we can indetify unimportant
parameters.  
One can attempt to reduce parameter dimension by fixing
unimportant parameters at nominal values. Suppose $\xi_i$ is
deemed unimportant as a result of a global sensitivity analysis
based on computing the activity scores. We want to understand
what is the approximation error that occurs once we fix $\xi_i$ 
at a nominal value.
%Let $\I \subset \{1, \ldots,
%Np\}$ be an index set that indexes the unimportant parameters in the parameter
%vector $\vec{\xi}$.  Letting $|\I|$ denote the number of elements of $\I$, 
%we denote by $\vec{z} \in \R^{|\I|}$ a generic vector of nominal values for
%the unimportant parameters $\vec\xi_\I$, and let $\vec{y}$ be a generic vector of
%important variables, $\vec\xi_{\I^c}$,
%where $\I^c$ denotes the complement of $\I$ in $\{1, \ldots, \Np\}$. 
%For a vector $\xi$, we let $\xin$ denote the vector obtained by removing
%$\xi_i$. 

Let $\vec\xi$ be given and let $z$ be a nominal value for $\xi_i$;  we define 
$\vec{y}^z(\vec\xi)$ as the vector with entries $y^z_j = \xi_j$ for $j \neq i$
and $y^z_i = z_i$.
We consider the \emph{reduced} model, 
obtained by fixing $\xi_i$ at a nominal value $z \in \R$: 
\[
f^{z}(\vec\xi) = f(\vec{y}^z(\vec\xi))
\] 
We consider the following error indicator.
\[
\delta(z) =
\frac{ \int_\Omega \big( f(\vec\xi) - f^{z}(\vec\xi)\big)^2 \, \mu(d\vec\xi) }
          {\int_\Omega f(\vec\xi)^2 \, \mu(d\vec\xi)}.
\] 

\begin{theorem}
We have $\Ez{ \delta(z)} \leq 2C_i\big(\act{f}{i}{r} + \lambda_{r+1}\big)/{\V(f)} $.
\end{theorem}
%
%\[
%       T_i(f) \leq \frac{C_i}{\V(f)}(\act{f}{i}{r} + \lambda_{r+1}).
%\]
%we obtain the following useful inequality, which
%links activity scores and Sobol' indices:
%for $1 \leq r \leq \Np$, and $i = 1, \ldots, \Np$, we have
%\[
%       T_i(f) \leq \frac{C_i}{\V(f)}(\act{f}{i}{r} + \lambda_{r+1}).
%\]
%This was noted in~\cite{Diaz:2016} for the specific case of all $\xi_i$'s being
%iid $U(0, 1)$, in which case $C_i = 1/\pi^2$, $i = 1, \ldots, \Np$.



