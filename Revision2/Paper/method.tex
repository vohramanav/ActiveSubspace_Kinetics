\section{Methodology}
\label{sec:method}

In this section, we outline the methodology for computing the
active subspace in an efficient manner. The proposed framework is employed to analyze 
a 19-dimensional H$_2$/O$_2$ reaction kinetics
problem whereby the logarithm of the 
pre-exponent ($A_i$) in the rate law associated with individual reactions provided in
Table~\ref{tab:kinetics} is considered to be uniformly
distributed in the interval, $[0.97\log(A_i^\ast), 1.03\log(A_i^\ast)]$;
$A_i^\ast$ is the nominal
estimate provided in~\cite{Yetter:1991}.
Two approaches are explored for estimating the gradient of ignition delay with
respect to $\log(A_i)$: 
a perturbation  approach that involves computation of model
gradients using finite difference in order to construct the matrix $\hat{\mat{C}}$
in~\eqref{eq:chat}, and a regression  approach 
that involves a linear regression fit to the available set of
model evaluations in order to approximate the gradient. 
The active subspace is computed in an iterative manner to avoid
unnecessary model evaluations once converged is established.

%\subsection{Gradient-based approach}
%\label{sub:grad}

As discussed earlier, gradient estimation using finite differences 
requires additional model evaluations at the
neighboring points in the input domain. 
Hence, for $N$ samples in a $d$-dimensional parameter space, $N(d+1)$
model evaluations are needed. On the other hand, gradient estimation
using the regression-based approach involves a series of linear regression
fits to subsets of available evaluations as discussed 
in~\cite[Algorithm 1.2]{Constantine:2015}. Hence, the computational effort is  
reduced by a factor $(d+1)$ when using the regression-based approach.
In other words, for the same amount of computational effort, the regression
approach can afford a sample size that is (d+1) times larger than that in the
case of perturbation approach.
The specific sequence of steps
for computing the active subspace is discussed as follows.   

We begin by evaluating the gradient of the model output, $\nabla_{\bm{\xi}}f$,
at an initial set of $n_0$ samples (generated using Monte Carlo sampling)
denoted by $\bm{\xi}_i$, $i$ = $1,\ldots,n_0$.
Using the gradient
evaluations, the matrix, $\hat{\mat{C}}$ is computed. Eigenvalue decomposition
of $\hat{\mat{C}}$ yields an initial estimate of the dominant eigenspace,
$\hat{\mat{W}}_1$ and the set of corresponding eigenvalues, 
$\hat{\mat{\Lambda}}_1$. Note that $\hat{\mat{W}}_1$ is obtained by
partitioning the eigenspace around $\lambda_j$ such that the ratio of
subsequent eigenvalues,
$\left(\frac{\lambda_j}{\lambda_{j+1}}\right)\ge\mathcal{O}(10^1)$.
 At each subsequent iteration, model evaluations are
generated at a new set of $n_k$ samples. The new set of gradient evaluations
are augmented with the available set to re-construct $\hat{\mat{C}}$ followed
by its eigenvalue decomposition. The relative change in the norm of the
difference in squared value of individual components of the dominant
eigenvectors between subsequent iterations is evaluated. The process is
terminated and the resulting eigenspace is considered to have converged once
the maximum relative change at iteration $k$, $\max(\delta \hat{\mat{W}}_{1,j}^{(k)})$
($j$ is used as an index for the eigenvectors),
is smaller than a given tolerance, $\tau$.  A regression fit to
$G(\hat{\mat{W}}_1^\top\bm{\xi})$ is used as a surrogate to characterize and
quantify the uncertainty in the model output. Moreover, the components of the
eigenvectors in the active subspace are used to compute the activity scores, $\bm{\nu}_r(f)$,
which provide an insight into the relative importance of the uncertain inputs.
Note that the index, $r$, corresponds to the number of eigenvectors in
$\hat{\mat{W}}_1$. The sequence of steps as discussed are outlined
in Algorithm~\ref{alg:grad}.

%% Grad-based algorithm

\bigskip
\begin{breakablealgorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{An iterative strategy for discovering the active subspace}
  \begin{algorithmic}[1]
\Require $\theta_l$, $\theta_u$, $\beta$, $\tau$. 
\Ensure $\hat{\mat{\Lambda}}$, $\hat{\mat{W}}$, $\bm{\nu}_r(f)$ %$\eta$. %
    \Procedure{Active Subspace Computation}{}
    \State Set $k$ = 0
	\State Draw $n_k$ random samples, $\{\bm{\xi}_i\}_{i=1}^{n_k}$ 
         according to $\pi_{\bm{\xi}}$. 
    \State Set $N_\text{total}$ = $n_k$ 
	\iffalse \State Project to the physical space:
        $\{\bm{\theta}_k\}_{k=1}^{n_r}=\theta_l+0.5(\theta_u-\theta_l)\{\bm{\xi}_k\}_{k=1}^{n_r}$ \fi
	\State For each $i=1, \ldots, N_\text{total}$, compute $f(\bm{\xi}_i)$ and the gradient $\bm{g}^i = \nabla_{\bm{\xi}}f(\bm{\xi}_i)$
    \iffalse  
	\Statex\hspace{5mm} Using Finite Difference: 
	\Statex\hspace{5mm} i. Assign a small increment, $d\xi$.
	\Statex\hspace{5mm} ii. Augment the set of samples with neighboring points.
	\be \{\bm{\Xi}_k\}_{k=1}^{n_1(N_p+1)}:~\{\bm{\xi}_k\}_{k=1}^{n_1} \cup
        \{\xi_{k,j}+d\xi\}_{j=1}^{N_p} \nonumber
	\ee
	\Statex\hspace{5mm} iii. Project to the physical space:
        $\{\bm{\theta}_k\}_{k=1}^{n_1(N_p+1)}=\theta_l+0.5(\theta_u-\theta_l)\{\bm{\Xi}_k\}_{k=1}^{n_1(N_p+1)}$
	\Statex\hspace{5mm} iv. Using the augmented set, $\{\bm{\theta}_k\}_{k=1}^{n_1(N_p+1)}$
         compute $\bm{g}^i$.
         \fi 
	\State Compute $\hat{\mat{C}}$ and its eigenvalue decomposition 
		$\hat{\mat{C}}$= $\frac{1}{N_\text{total}}\sum\limits_{i=1}^{N_\text{total}}[\bm{g}^i][\bm{g}^i]^\top$ = 
		$\hat{\mat{W}}^{(k)}\hat{\mat{\Lambda}}^{(k)} \hat{\mat{W}}^{(k)\top}$
	%\State Eigenvalue decomposition, $\mat{C}$ = $W^{(0)}\Lambda^{(0)} W^{(0)\top}$%
	\State Partition: $\hat{\mat{\Lambda}}^{(k)}=
        \begin{bmatrix} \hat{\mat{\Lambda}}_1^{(k)} & \\ & \hat{\mat{\Lambda}}_2^{(k)} \end{bmatrix}$, 
        $\hat{\mat{W}}^{(k)}=\begin{bmatrix} \hat{\mat{W}}_1^{(k)} & \hat{\mat{W}}_2^{(k)} \end{bmatrix}$, 
        $\hat{\mat{\Lambda}}_1^{(k)}\in \mathbb{R}^{N_p\times r}$
	\Loop
		\State Set $k$ = $k$ + 1
		\State Draw $n_k =  \lceil\beta n_{k-1}\rceil$  new random samples 
                $\{\bm{\xi}_i\}_{i=1}^{n_k}$  $\beta\in[0,1]$
                
	%	\State Project $\bm{\xi}_k$~$\rightarrow$~$\bm{\theta}_k$.%
		\State Set $N_\text{total}$ = $N_\text{total}$ + $n_k$ 
		\State Compute $\bm{g}^i = \nabla_{\bm{\xi}_i}f(\bm{\xi}_i)$, 
             	$i=n_{k-1}+1, \ldots, n_{k-1}+n_k$.  
		\State Compute $\hat{\mat{C}}$ = 
        	$\frac{1}{N_\text{total}}\sum\limits_{k=1}^{N_\text{total}}[\bm{g}^i][\bm{g}^i]^\top$
		\State Eigenvalue decomposition, $\hat{\mat{C}}$ = $\hat{\mat{W}}^{(k)}\hat{\mat{\Lambda}}^{(k)}
		 \hat{\mat{W}}^{(k)\top}$
		\State Partition the eigenspace of $\hat{\mat{C}}$ as shown in Step 7
		\State Compute $\delta \hat{\mat{W}}_{1,j}^{(k)}$ = 
                       \scalebox{1.25}{$\frac{\|(\hat{\mat{W}}_{1,j}^{k})^2 - 
                       (\hat{\mat{W}}_{1,j}^{k-1})^2\|_2}{\|(\hat{\mat{W}}_{1,j}^{k-1})^2\|_2}$}, 
                       $j = 1,\ldots,r$.
		\If {$\max\limits_{j}\left(\delta \hat{\mat{W}}_{1,j}^{(k)}\right)<\tau$}
			\State break
		\EndIf
	\EndLoop
	\State Compute $\nu_{i,r}(f) = \sum\limits_{j=1}^{r} \lambda_j w_{i,j}^2$,
	$i=1,\ldots,N_p$.
	\State Normalize $\nu_{i,r}(f)$ as $\tilde{\nu}_{i,r}(f)$ = \scalebox{1.25}{$\frac{\nu_{i,r}(f)}{\sum_i\nu_{i,r}(f)}$}.
	
    \EndProcedure
  \end{algorithmic}
  \label{alg:grad}
\end{breakablealgorithm}
\bigskip

To assess its feasibility and suitability, we implement
Algorithm~\ref{alg:grad} to compute the active subspace for 
the 19-dimensional H$_2$/O$_2$ reaction kinetics
problem by perturbing $\log(A_i)$ by 3$\%$ about its nominal
value as discussed earlier. For the purpose of verification,
$\hat{\mat{C}}$ was initially constructed using a large set of samples
($N$~=~1000) in the input domain. The gradient was estimated using
finite difference, and hence, a total of 20,000 model runs were performed. 
In Figure~\ref{fig:eig_comp}, we illustrate the comparison of
the resulting normalized eigenvalue spectrum by plotting 
\rebut{$(\lambda_i/\lambda_0)$}
($i = 1,\ldots,19$) corresponding to $N$~=~1000 and the same quantity corresponding to
a much smaller set of samples, $n$~=~$\{20,40,80,120\}$.
%
\begin{figure}[htbp]
 \begin{center}
  \includegraphics[width=0.45\textwidth]{./Figures/eig_comp_p3}
%   \includegraphics[width=0.48\textwidth]{./Figures/err_eigv_1}
\caption{A comparison of the normalized eigenvalue spectrum, 
\rebut{($\lambda_i/\lambda_0)$}
using $n$~=~$\{20,40,80,120\}$ samples with that
obtained using a much larger sample size, $N$~=~1000. 
%Right: Relative $L_2$ 
%norm of the difference between
%the square of individual components ($w_i$'s) of the dominant eigenvector evaluated using 
%$N$=1000 and $n$~=~$\{20,40,80,120\}$ samples.
} 
\label{fig:eig_comp}
\end{center}
\end{figure}
%
We observe that the dominant eigenvalues, $\lambda_1, \ldots, \lambda_4$, 
are approximated 
reasonably well with just 20 samples. As expected, the accuracy of higher-index
 eigenvalues is observed
to improve with the sample size. Since 
$\lambda_1$ is roughly an order of magnitude larger than $\lambda_2$, we expect 
a 1-dimensional active subspace to reasonably approximate the uncertainty
in the ignition delay. 
To further confirm this, we evaluate a relative L$^2$ norm of the difference
 ($\varepsilon_{\text{L}^{2}}^{N-n}$) between the 
squared value of corresponding components of the dominant eigenvector, computed using $N$~=~1000 ($\vec{w}_{1,N}$)
and $n$~=~$\{20,40,80,120\}$ ($\vec{w}_{1,n}$) as follows:
%
\be
\varepsilon_{\text{L}^{2}}^{N-n} = 
\frac{\|\vec{w}_{1,N}^2-\vec{w}_{1,n}^2\|_2}{\|\vec{w}_{1,N}^2\|_2}
\label{eq:accu}
\ee
%
The quantity, $\varepsilon_{\text{L}^{2}}^{N-n}$, was found to be 
$\mathcal{O}(10^{-2})$ in all cases.
Thus, even a small sample size, $n$ = 20, seems to approximate the dominant eigenspace with
reasonable accuracy in this case. 
The iterative strategy therefore offers a significant potential for computational gains. 

%\subsection{Gradient-free approach}
%\label{sub:gradfree}

The active subspace for the 19-dimensional problem was also computed using regression-based
estimates of the gradient that do not require model evaluations at neighboring points as
discussed earlier.
%
%The gradient-free approach exploits a regression-based local linear
%approximation to the model output for estimating the gradients, $\bm{g}^i$
%in Algorithm~\ref{alg:grad}. Once again, we use an iterative strategy for
%computing the active subspace for the 19-dimensional 
%H$_2$/O$_2$ reaction kinetics problem considered in~\ref{sub:grad}.
%Moreover, we compare our findings with
%those obtained using the gradient-based approach to assess consistency
%between the two approaches. 
%
%In the gradient-free approach, the model output is first computed at an initial set of $n_0$ samples. 
%An independent set of $M$ samples is also drawn according to the joint probability distribution of the inputs.
%For each sample in $M$, $p$ nearest neighbors in the $n_0$ set are identified. A least-squares fit to model
%evaluations at the $p$ samples is performed. The slope vector ($\bm{b}_i$) of the regression fit is stored
%separately and the process is repeated for each sample in $M$. The symmetric and positive semi-definite matrix,
%$\hat{\mat{C}}$ is constructed using the slope vectors as follows:
%%
%\be
%\hat{\mat{C}} = \frac{1}{M}\sum\limits_{i=1}^{M}\bm{b}_i\bm{b}_i^\top = \hat{\bm{W}}\hat{\bm{\Lambda}}\hat{\bm{W}}^\top
%\ee
%%
%Hence, the gradient-free approach essentially differs in the procedure for constructing the matrix, $\hat{\mat{C}}$.
%Instead of the model evaluations, a linear regression-based approximation is used to estimate the gradient of
%the model output. The suitability of this approach thus depends upon the accuracy of this approximation. 
%The active subspace computation involves the same sequence of steps as outlined previously for the gradient-based 
%approach in Algorithm~\ref{alg:grad}. The procedure for constructing $\hat{\mat{C}}$ using a regression-based
%local linear approximation
%of the model output is provided in Algorithm~\ref{alg:free2}.
%
%\bigskip
%\begin{breakablealgorithm}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}
%  \caption{An iterative gradient-free approach for discovering the active subspace}
%  \begin{algorithmic}[1]
%\Require $\theta_l$, $\theta_u$, $M$, $\tau$. 
%\Ensure $\Lambda$, $W$, $\alpha$ %$\eta$. 
%    \Procedure{Gradient-free}{}
%    \State Set $k$ = 0
%	\State Draw $n_k$ random samples, $\{\bm{\xi}_j\}_{j=1}^{n_k}$
%	\State Set $N_\text{total}$ = $n_k$ 
%	\iffalse \State Project to the physical space:
%        $\{\bm{\theta}_k\}_{i=1}^{n_k}=\theta_l+0.5(\theta_u-\theta_l)\{\bm{\xi}_k\}_{k=1}^{n_r}$
%        \fi
%     \State Draw $M$ random samples, $\{\bm{\nu}_i\}_{i=1}^{M}$ 
%	\State Compute $f(\bm\xi_j)$, $j=1, \ldots, N_\text{total}$.
%	\State Compute the matrix $\mathbb{C}$ and its eigenvalue decomposition using Algorithm 3
%	%\State Eigenvalue decomposition, $\mat{C}$ = $W^{(r)}\Lambda^{(r)} W^{(r)\top}$
%	\State Partition: $\Lambda^{(k)}~=~ 
%        \begin{bmatrix} \Lambda_1^{(k)} & \\ & \Lambda_2^{(k)} \end{bmatrix}$, 
%        $W^{(k)}~=~\begin{bmatrix} W_1^{(k)} & W_2^{(k)} \end{bmatrix}$, 
%        $\Lambda_1^{(k)}\in \mathbb{R}^{d\times\mathcal{S}}$
%	\Loop
%		\State Set $k$ = $k$ + 1
%		\State Draw $n_k =  \beta n_{k-1}$  new random samples 
%		$\{\bm{\xi}_j\}_{j=1}^{n_k}$  $\beta =??$  $j = n_{k-1}+1,\ldots,n_{k-1}+n_k$. 
%        \State $N_\text{total}$ = $N_\text{total}$ + $n_k$    
%	\iffalse	\State Project $\bm{\xi}_k$~$\rightarrow$~$\bm{\theta}_k$. \fi
%		 
%		\State Compute $f(\bm\xi_j)$, $j=n_{k-1}+1, \ldots, n_{k-1}+n_k$.  
%		\State Construct the matrix, $\mat{C}$ and its eigenvalue decomposition
%		\State Compute $\delta W_{1,j}^{(k)}$ = 
%                       \scalebox{1.25}{$\frac{\|(W_{1,j}^{k})^2 - 
%                       (W_{1,j}^{k-1})^2\|}{\|(W_{1,j}^{k-1})^2\|}$}, 
%                       $j = 1,\ldots,\mathcal{S}$.
%		\If {$\max\left(\delta W_{1,j}^{(k)}\right)<\tau$}
%			\State break
%		\EndIf
%	\EndLoop
%	\State Compute $\alpha_i(N_\text{total}) = \sum\limits_{j=1}^{\mathcal{S}} \Lambda_{1_{i,j}}W_{1_{i,j}}^2$,
%	$i=1,\ldots,d$.
%    \EndProcedure
%  \end{algorithmic}
%  \label{alg:free1}
%\end{breakablealgorithm}
%\bigskip
%
%\bigskip
%\begin{breakablealgorithm}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}
%  \caption{Algorithm for constructing the matrix, $\hat{\mat{C}} in~\eqref{eq:chat}$}
%  \begin{algorithmic}[1]
%	\Procedure{Local Linear Approximation}{} 
%	\State Draw $N$ random samples, $\{\bm{\xi}_j\}_{j=1}^{N}$ 
%	according to $\pi_{\bm{\xi}}$.
%	\iffalse \State Project to the physical space:
%	$\{\bm{\theta}_k\}_{k=1}^{N}=\theta_l+0.5(\theta_u-\theta_l)\{\bm{\xi}_k\}_{k=1}^{N}$ \fi
%	\State Compute $f(\bm\xi_j)$, $j=1, \ldots, N$.
%	\State Draw $M$ random samples, $\{\bm{\zeta}_i\}_{i=1}^{M}$
%	according to $\pi_{\bm{\xi}}$.
%	\State Choose an integer $p \leq N$ 
%	\State For each $i=1, \ldots, M$, compute 
%	\[
%	\begin{aligned}
%	\Phi_i &= \{ p \text{ nearest points in } \{\bm{\xi}_j\}_{j=1}^{N} \text{ to } \bm{\zeta}_i\}\\
%	\vspace{-2mm}
%	\Psi_i &= \text{subset of } f(\bm\xi_j) \text{ corresponding to the points in } \Phi_i
%	\end{aligned}
%	\]
%	\State Compute linear least square fits 
%	 $f(\bm\xi_j) \approx c_i + b_i^T\bm{\xi}_j$, \  $\bm{\xi}_j \in \Phi_i, \ f(\bm\xi_j) \in \Psi_i$
%	\State Compute the matrix 
%	\[
%	\hat{\mat{C}} \approx \frac{1}{M} \sum_{i=1}^{M} b_i b_i^T = \hat{\mat{W}}\hat{\mat{\Lambda}}\hat{\mat{W}}^\top
%	\]
%	\EndProcedure
%  \end{algorithmic}
%  \label{alg:free2}
%\end{breakablealgorithm}
%\bigskip
%
The quantity, $\rebut{\max\limits_j}(\delta \hat{\mat{W}}_{1,j}^{(r)})$ defined in Algorithm~\ref{alg:grad} was
used to assess the convergence behavior of the two approaches. Using a set tolerance, $\tau$~=~0.05,
it was observed that both perturbation and regression approaches took 8 iterations to converge. 
Note that the computational effort at each iteration was considered to be the same in both cases.
More specifically, 5 new random samples were added for the perturbation approach at each iteration.
However, as discussed earlier, \rebut{a} total of 100 (=5$\times$(19+1)) model runs were needed to obtain the
model prediction and its gradients at these newly generated samples. Hence, in the case of
regression, 100 new random samples were generated at each iteration since gradient computation does
not require additional model runs in this case. Thus, including the initial step, a total of 900
model runs were required to obtain a converged active subspace in both cases. 

The accuracy of the two approaches was assessed by estimating $\varepsilon_{\text{L}^{2}}^{N-n}$ 
using the components of the dominant eigenvector in the converged active subspace in each case
in~\eqref{eq:accu}. The quantity, $\varepsilon_{\text{L}^{2}}^{N-n}$ was estimated to be 0.0657
and 0.1050 using perturbation and regression respectively. Hence, the regression approach was
found to be relatively more accurate. Squared values of the individual components of the 
dominant eigenvector from the
two approaches and for the case using $N$~=~1000 in the perturbation approach are plotted in
Figure~\ref{fig:comp}~(left).
%
%is plotted in
%Figure~\ref{fig:comp} (center) to illustrate convergence characteristics based on Algorithm~\ref{alg:grad} 
%for the two approaches. As observed for a set tolerance $\tau$~=~0.05, 8 iterations are needed 
%to converge in both cases. 
%At each iteration, 5 new random samples were added for the perturbation approach. As discussed earlier, gradient 
%computation (using finite difference) at 5 samples requires 100 model runs. Hence, in order to compare the
%two approaches for the same computational effort, we generate 100 random samples in the 19D input domain for 
%the regression approach. Therefore, 8 iterations require 800 model runs leading to a total of 900 model runs 
%including the initial step in both cases.  The accuracy of the two approaches is compared for the same 
%computational effort by computing $\varepsilon_{\text{L}^{2}}^{N-n}$ using~\eqref{eq:accu} in 
%Figure~\ref{fig:comp} (right). Once again, $N$
%corresponds to the case of active subspace computation using the perturbation approach using 1000 samples
%in the 19D domain; $n$ ranges from 5 to 45 for the perturbation approach, and 100 to 900 for the regression 
%approach corresponding to iterations 1 to 8. The accuracy of the two approaches is found to be comparable for
%the same amount of computational effort. However, the perturbation approach is observed to be slightly more
%accurate.}
%
%\begin{figure}[htbp]
% \begin{center}
%  \includegraphics[width=0.8\textwidth]{./Figures/eig_convD19}
%\caption{\rebut{Left: An illustrative comparison of individual squared components of the converged dominant
%eigenvector obtained using perturbation and regression strategies using $M$~=~900 model runs in each case. 
%Additionally, the dominant eigenvector components obtained using $M$~=~20000 model runs (corresponding to 
%$N$~=~1000 samples)
%in the perturbation strategy (test case), used to assess the accuracy of the two strategies are also plotted.
%Center: The quantity,  $\max(\delta \hat{\mat{W}}_{1,j}^{(r)})$
%is plotted for successive iterations to illustrate convergence behavior for the two strategies. Right: The
%quantity, $\varepsilon_{\text{L}^{2}}^{N-n}$ is plotted as a function of iterations to compare the accuracy
%of the two strategies. Note that the above plots compare the convergence and accuracy of the two
%strategies for the same amount of computational effort, quantified by the number of model runs, $M$.}}
%\label{fig:comp}
%\end{center}
%\end{figure}
%
\begin{figure}[htbp]
 \begin{center}
  \includegraphics[width=0.38\textwidth]{./Figures/eig_conv19D}
  \includegraphics[width=0.45\textwidth]{./Figures/comp_ssp19D}
\caption{Left: An illustrative comparison of individual squared components of the converged dominant
eigenvector obtained using perturbation and regression strategies using $M$~=~900 model runs in each case. 
Additionally, the dominant eigenvector components obtained using $M$~=~20000 model runs (corresponding to 
$N$~=~1000 samples)
in the perturbation strategy (test case), used to assess the accuracy of the two strategies are also plotted.
Right: An illustrative comparison of the SSPs generated using the
perturbation and the regression strategies for computing the active subspace. 
}
\label{fig:comp}
\end{center}
\end{figure}
%
The set of eigenvector components for the three cases are found to be in excellent agreement with
each other, indicating that both
approaches have sufficiently converged and are reasonably accurate for this setup.

As mentioned earlier, the model output $f(\bm{\xi})$ i.e. the ignition delay in the
H$_2$/O$_2$ reaction in this case, varies
 predominantly in a 1-dimensional active subspace. Hence, 
$f(\bm{\xi})$ can be approximated as $G(\hat{\mat{W}}_1^\top\bm{\xi})$ in
the 1-dimensional active subspace. The plot of $G$ versus $\hat{\mat{W}}_1^\top\bm{\xi}$, 
regarded as the \textit{sufficient summary plot} (SSP), obtained using the perturbation-based and
regression-based gradient estimates are
compared in Figure~\ref{fig:comp}~(right).
%
%\begin{figure}[htbp]
% \begin{center}
%  \includegraphics[width=0.45\textwidth]{./Figures/comp_ssp19D}
%\caption{\rebut{An illustrative comparison of the SSPs generated using the 
%perturbation and the regression strategies for computing the active subspace.}}
%\label{fig:comp_ssp}
%\end{center}
%\end{figure}
%
The dominant eigenvector obtained using perturbation is based on
$N$~=~45 samples which requires $M$~=~900 model runs. For the same amount of
computational effort, we can afford $N$~=~900 samples when using regression.
Hence, the SSP from regression is based on 900 points:
($\hat{\mat{W}}_1^\top\bm{\xi}_j$,~$G(\hat{\mat{W}}_1^\top\bm{\xi}_j)$),
$j = 1, \ldots, 900$. On the
other hand, the SSP from perturbation is plotted using only 45 points as
mentioned earlier.  Nevertheless, the illustrative comparison clearly indicates
that the two SSPs are in excellent agreement. Moreover, it is interesting to
note that the response in ignition delay based on the considered probability
distributions for $\log(A_i)$ although non-linear, can be approximated by a
1-dimensional active subspace.

We further estimate
the normalized activity scores for individual uncertain inputs ($\tilde{\nu}_{i,r}$; $r$=1 since a
1-dimensional active subspace seems reasonably accurate)
using the components of the dominant eigenvector as shown in Algorithm~\ref{alg:grad} (steps 21 and 22).
The activity scores for the 19 uncertain pre-exponents ($A_i$'s), estimated
using the perturbation and regression strategies are plotted in Figure~\ref{fig:comp_as}.  
%
\begin{figure}[htbp]
 \begin{center}
  \includegraphics[width=0.45\textwidth]{./Figures/comp_as19D}
%  \includegraphics[width=0.45\textwidth]{./Figures/ub_conv_kinetics_rich}
\caption{Left: A bar-graph of normalized activity scores ($\tilde{\nu}_{i,r}$'s) 
for the 19 uncertain pre-exponents ($A_i$'s); $r$
denotes the number of eigenvectors in the dominant eigenspace.}
\label{fig:comp_as}
\end{center}
\end{figure}
%
The activity scores based on the two approaches for gradient estimation agree favorably 
with each other as well as those
based on the screening metric involving the DGSMs in~\cite{Vohra:2018}. It is observed that the uncertainty
associated with the ignition delay is largely due to the uncertainty in $A_9$ while $A_1$, $A_{15}$, and
$A_{17}$ are also observed to contribute significantly towards its variance.

The above comparisons indicate that the gradient of the ignition delay with respect to the
uncertain $A_i$'s is reasonably approximated using both, perturbation  and regression 
approaches in this case. Since both approaches yield consistent results and are
comparable in terms of convergence and accuracy, we could use either for the purpose
of active subspace computation for this setting.
In the following section, we shift our focus to 
the higher-dimensional H$_2$/O$_2$ reaction kinetics application wherein the activation
energies in the rate law as well as initial pressure, temperature, and stoichiometric conditions
are also considered to be uncertain.
